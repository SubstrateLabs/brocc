## recommendation

- https://docs.unstructured.io

## future

- later try semantic chunking: https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/

In summary, the trend is toward chunking that understands the content, whether via embeddings (semantic similarity), large language models (for topic or discourse cues), or vision/layout models (for document structure). The end goal is the same: produce chunks that align with human-understandable units of meaning, thus optimizing them for embedding and retrieval.
• Proposition-Based Chunking: Researchers have proposed chunking text into propositions – minimal, self-contained factual statements. A 2023 study by Chen et al. introduced propositions as a retrieval unit and even fine-tuned a “Propositionizer” model to split text accordingly ￼ ￼. Each proposition encapsulates a single idea, with all necessary context (e.g. coreferences resolved) so that it stands alone ￼ ￼. For example, “I love dogs. They are amazing.” would be transformed into “I love dogs. Dogs are amazing.”, making each sentence independent ￼ ￼. This granular approach showed improved dense retrieval performance over sentence or passage chunking ￼. While proposition-level chunks may be too fine for some applications, the concept is influencing tools – we see echoes of this in LlamaIndex’s approach of using an LLM to ensure self-contained chunks ￼. It wouldn’t be surprising if future versions of LangChain or others allow an option to “propositionize” text via an API call to a model like chentong00/propositionizer on HuggingFace, especially for Q&A use cases that demand high precision.
• Rhetorical Structure & Discourse Chunking: Another angle is using discourse analysis (e.g. Rhetorical Structure Theory, RST) to segment text. RST posits that any well-written document can be parsed into a hierarchical discourse tree of units and relations ￼. Leaves of this tree are Elementary Discourse Units (small clauses or sentences) and higher nodes represent larger segments like nested clauses, sentences, or paragraphs bound by rhetorical relations (contrast, elaboration, cause-effect, etc.) ￼ ￼. In theory, one could chunk a document along these discourse units, so that each chunk represents a rhetorically complete piece of information (for instance, a cause with its effect, or a claim with its supporting evidence). This is a semantically informed method that goes deeper than surface cues. However, performing RST parsing requires specialized models, and while there are open-source RST parsers, they have not yet been widely packaged into RAG pipelines. There is ongoing research to improve discourse parsing (including multilingual RST parsing ￼), which could eventually feed into smarter chunkers. For now, RST remains more in the academic realm, but it’s conceivable that LLMs themselves could be prompted to do a form of discourse segmentation. We already see LLMs used to decide chunk merges in LlamaIndex, which is a simple step in that direction.
